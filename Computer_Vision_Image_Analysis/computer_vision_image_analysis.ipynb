{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CosiMichele/uadl-cv/blob/main/workshop-content/uadl-cv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQeuFnz784BZ"
      },
      "source": [
        "# Computer Vision: Image Analysis\n",
        "\n",
        "In this notebook, we cover the basics of Computer Vision, first by showing a classical approach using OpenCV, then through a modern take using CNN. To help with the tutorial, we will be using the following images. \n",
        "\n",
        "`apples_clear.png`\n",
        "<div>\n",
        "<img src=\"https://www.kew.org/sites/default/files/styles/social/public/2022-05/apple%20cultivars.png\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "`apples_board.png`\n",
        "<div>\n",
        "<img src=\"https://media.post.rvohealth.io/wp-content/uploads/2020/09/health-benefits-of-apples-1200x628-facebook-1200x628.jpg\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "`apples_table.png`\n",
        "<div>\n",
        "<img src=\"https://i0.wp.com/blog.blueapron.com/wp-content/uploads/2020/09/fall_apple_varieties-1.jpg?fit=1920%2C1228&ssl=1\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "## Table of contents\n",
        "\n",
        "- Prerequisites\n",
        "- A Classical Approach: OpenCV\n",
        "- OpenCV Exercise\n",
        "- A Modern Approach: CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "Download the images you require for this exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "002G1zWK84Be",
        "outputId": "9fe8b1c5-ad62-45ca-be3e-7e91195bdc94"
      },
      "outputs": [],
      "source": [
        "# Downloading images \n",
        "!wget -O apples_clear.png https://www.kew.org/sites/default/files/styles/social/public/2022-05/apple%20cultivars.png?h=1d19dfc9&itok=NT2TwltK\n",
        "!wget -O apples_board.png https://media.post.rvohealth.io/wp-content/uploads/2020/09/health-benefits-of-apples-1200x628-facebook-1200x628.jpg\n",
        "!wget -O apples_table.png https://i0.wp.com/blog.blueapron.com/wp-content/uploads/2020/09/fall_apple_varieties-1.jpg?fit=1920%2C1228&ssl=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load all the libraries necessary. This notebook is designed to run seamlessly on both Colab and your personal computer. The necessary libraries will be loaded in the following cell, accompanied by a note specifying which libraries are required for each section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect if notebook is running on Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    \n",
        "# Load remaining required packages\n",
        "## Packages required for the classical approach\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "## Packages required for the modern approach\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from PIL import Image, ImageDraw\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Code for Colab environment\n",
        "    from google.colab.patches import cv2_imshow\n",
        "    print(\"All the libraries have loaded for a colab system.\")\n",
        "else:\n",
        "    # Code for local environment\n",
        "    print(\"All the libraries have loaded for your local system.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A Classical Approach: OpenCV\n",
        "\n",
        "In this section we are going to be using OpenCV to count apples.  To achieve this, OpenCV is used in such a manner where the edges of an object are detected first and then the object is counted. This process involves the following steps:\n",
        "\n",
        "1. Convert image to black and white. Since our goal is to count still objects in an image, the conversion to black and white helps with removing not needed features (colors).\n",
        "2. Blurring the image using Gaussian Blur: this helps with the reduction of noise in the image\n",
        "3. Finding the edges of the blurred objects using the Canny edge detector\n",
        "4. Finding and counting the contours of the objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "tMwK4mUF84Bh",
        "outputId": "35445281-c4ae-408e-c4f6-f5e4ae56aa6a"
      },
      "outputs": [],
      "source": [
        "# import cv2                                    # These are the required libraries for this section\n",
        "# import numpy as np                            # If running on your computer, all you need is cv2 and np!\n",
        "# from google.colab.patches import cv2_imshow   # If you're running on colab, you will also need this\n",
        "\n",
        "# 0. Load the image of apples\n",
        "image = cv2.imread('apples_clear.png')\n",
        "\n",
        "# Visualize image if needed \n",
        "if IN_COLAB:\n",
        "    cv2_imshow(image)\n",
        "else:\n",
        "    cv2.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "hW0Y_EjA9eld",
        "outputId": "cf339b64-f3f4-40b4-b174-852d9249cb17"
      },
      "outputs": [],
      "source": [
        "# 1. Convert the image to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Visualize image if needed \n",
        "if IN_COLAB:\n",
        "    cv2_imshow(gray)\n",
        "else:\n",
        "    cv2.imshow(gray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "0y1o_QfA9wHn",
        "outputId": "cb9d4906-dad9-45f5-cd9c-caa136f6bddb"
      },
      "outputs": [],
      "source": [
        "# 2. Apply Gaussian blur to reduce noise\n",
        "ksizeX = 7\n",
        "ksizeY = 7\n",
        "blurred = cv2.GaussianBlur(gray, (ksizeX, ksizeY), 0)\n",
        "\n",
        "# 3. Perform edge detection\n",
        "edges = cv2.Canny(blurred, 40, 100)\n",
        "\n",
        "# Visualize image if needed \n",
        "if IN_COLAB:\n",
        "    cv2_imshow(edges)\n",
        "else:\n",
        "    cv2.imshow(edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "K0ZJHqv7-COt",
        "outputId": "e2ec07de-fcf3-40ef-cb92-1bedb14b5673"
      },
      "outputs": [],
      "source": [
        "# 4. Find contours in the edge-detected image\n",
        "contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Initialize a counter for the number of apples\n",
        "apple_count = 0\n",
        "\n",
        "# Iterate through the contours and filter out small ones (potential noise)\n",
        "valid_contours = []\n",
        "for contour in contours:\n",
        "    if cv2.contourArea(contour) > 50:\n",
        "        valid_contours.append(contour)\n",
        "        apple_count += 1\n",
        "valid_contours = np.asarray(valid_contours, dtype=object)\n",
        "\n",
        "# Print the number of apples found\n",
        "print(f\"Number of apples: {apple_count}\")\n",
        "\n",
        "# Visualize the countours\n",
        "show_contours = np.stack((edges, edges, edges), axis=-1)\n",
        "\n",
        "for contour in valid_contours:\n",
        "  for [point] in contour:\n",
        "    show_contours[point[1], point[0], :] = [0, 0, 255]\n",
        "\n",
        "# Visualize image if needed \n",
        "if IN_COLAB:\n",
        "    cv2_imshow(show_contours)\n",
        "else:\n",
        "    cv2.imshow(show_contours)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenCV Exercise\n",
        "\n",
        "Try it out! See if you can count apples using the other images (`apples_board.png` or `apples_table.png`, the latter is **strongly** recommended)!\n",
        "\n",
        "Following are the cells provided for the exercise. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0. Load the image of apples\n",
        "image = cv2.imread('<image_you_want_to_load.png>')\n",
        "\n",
        "# Visualize image if needed \n",
        "if IN_COLAB:\n",
        "    cv2_imshow(image)\n",
        "else:\n",
        "    cv2.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Convert the image to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Visualize image if needed \n",
        "if IN_COLAB:\n",
        "    cv2_imshow(gray)\n",
        "else:\n",
        "    cv2.imshow(gray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Apply Gaussian blur to reduce noise\n",
        "ksizeX = 7                                              # These two values are the kerlel size: these can be negative, but these have to be an odd number \n",
        "ksizeY = 7                                              # The larger the numbers, the higher the smoothing effect is applied\n",
        "blurred = cv2.GaussianBlur(gray, (ksizeX, ksizeY), 0)   # 0 is the value of sigmaX: the standard deviation of the Gaussian kernel in the X direction. If it is set to 0, OpenCV calculates it based on the kernel size. \n",
        "\n",
        "# Visualize image if needed \n",
        "if IN_COLAB:\n",
        "    cv2_imshow(blurred)\n",
        "else:\n",
        "    cv2.imshow(blurred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Perform edge detection\n",
        "edges = cv2.Canny(blurred, 40, 100) # You may find it useful to change these values\n",
        "                                    # The second value is the low threshold, the third is the high threshold\n",
        "                                    # the low or high values are tied to the intensity of the edges\n",
        "# Visualize image if needed \n",
        "if IN_COLAB:\n",
        "    cv2_imshow(edges)\n",
        "else:\n",
        "    cv2.imshow(edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Find contours in the edge-detected image\n",
        "contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Initialize a counter for the number of apples\n",
        "apple_count = 0\n",
        "\n",
        "# Iterate through the contours and filter out small ones (potential noise)\n",
        "valid_contours = []\n",
        "for contour in contours:\n",
        "    if cv2.contourArea(contour) > 50:                       # You can play around with this value to establish what a contour is                                  \n",
        "        valid_contours.append(contour)\n",
        "        apple_count += 1\n",
        "valid_contours = np.asarray(valid_contours, dtype=object)\n",
        "\n",
        "# Print the number of apples found\n",
        "print(f\"Number of apples: {apple_count}\")\n",
        "\n",
        "# Visualize the countours\n",
        "show_contours = np.stack((edges, edges, edges), axis=-1)\n",
        "\n",
        "for contour in valid_contours:\n",
        "  for [point] in contour:\n",
        "    show_contours[point[1], point[0], :] = [0, 0, 255]\n",
        "\n",
        "# Visualize image if needed \n",
        "if IN_COLAB:\n",
        "    cv2_imshow(show_contours)\n",
        "else:\n",
        "    cv2.imshow(show_contours)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A Modern Approach: CNN\n",
        "\n",
        "As you may have figured out by this point, using OpenCV to count objects isn't simple and not always reliable. Modern methods like CNN can help speed up the application process, however these do require training. \n",
        "\n",
        "In this next section we are going to be using pre-trained weights, meaning that we do not need to train a model.\n",
        "\n",
        "The steps that are taken here are:\n",
        "\n",
        "1. Loading the model and setup preprocessing of images\n",
        "2. Write a function to perform detection on an image using torch and the pre-trained model\n",
        "3. Write a function that creates boxes and labels\n",
        "4. Wrote a function that shows the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "kcYYF3MC84Bi",
        "outputId": "9f64c5f7-2a56-49c4-deec-102b3de56138"
      },
      "outputs": [],
      "source": [
        "# import torch                                                          # These are the libraries required for this section\n",
        "# import torchvision\n",
        "# from torchvision import transforms\n",
        "# from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "# from PIL import Image, ImageDraw\n",
        "# import requests\n",
        "# from io import BytesIO\n",
        "\n",
        "# Load the pre-trained Faster R-CNN model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Modify the transformation for input images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to perform object detection on an image\n",
        "def detect_objects(model, image, threshold=0.5):\n",
        "    # Transform the input image\n",
        "    input_image = transform(image).unsqueeze(0)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        prediction = model(input_image)\n",
        "\n",
        "    # Filter out predictions below the threshold\n",
        "    boxes = prediction[0]['boxes'][prediction[0]['scores'] > threshold]\n",
        "    scores = prediction[0]['scores'][prediction[0]['scores'] > threshold]\n",
        "    labels = prediction[0]['labels'][prediction[0]['scores'] > threshold]\n",
        "\n",
        "    return boxes, scores, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to draw bounding boxes and labels on the image\n",
        "def draw_boxes(image, boxes, scores, labels):\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    for box, score, label in zip(boxes, scores, labels):\n",
        "        draw.rectangle([box[0], box[1], box[2], box[3]], outline=\"red\", width=3)\n",
        "        draw.text((box[0], box[1]), f\"Label: {label}, Score: {score:.2f}\", fill=\"red\")\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to visualize the detection results\n",
        "def visualize_results(image, boxes, scores, labels):\n",
        "    image_with_boxes = draw_boxes(image, boxes, scores, labels)\n",
        "    display(image_with_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example image path\n",
        "image_path = \"apples_table.png\"\n",
        "\n",
        "# Load image and perform object detection\n",
        "image = Image.open(image_path)\n",
        "\n",
        "boxes, scores, labels = detect_objects(model, image, threshold=0.5)\n",
        "\n",
        "# Visualize the results\n",
        "visualize_results(image, boxes, scores, labels)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
